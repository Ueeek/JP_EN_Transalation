{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os\n",
    "sys.path.append(\"/home/ueki.k/jp_en_translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from models.Seq2Seq_1 import build_model\n",
    "from utils.LangEn import LangEn\n",
    "from utils.LangJa import LangJa\n",
    "from utils.preprocess import loadLangs\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"corpus_file\":\"../data/jpn.txt\",\n",
    "    \"en_col\":\"description_en\",\n",
    "    \"jp_col\":\"description_jp\",\n",
    "    \"SOS_token\":1,\n",
    "    \"EOS_token\":0,\n",
    "    \"UNK_token\":2,\n",
    "    \"max_features\":5000,\n",
    "    \"MAX_LENGTH\":20,\n",
    "    \"train_size\":15000,\n",
    "    \"val_size\":100,\n",
    "    \"batch_size\":128,\n",
    "    \"epochs\":1,\n",
    "    \"maxlen_enc\":20,\n",
    "    \"maxlen_dec\":20,\n",
    "    \"n_hidden\":400,\n",
    "    \"input_dim\":5000,\n",
    "    \"output_dim\":5000,\n",
    "    \"emb_dim\":300,\n",
    "    \"use_enc_emb\":False,\n",
    "    \"use_dec_emb\":False,\n",
    "    \"validation_split\":0.1,\n",
    "    \"trained_param_dir\":\"../trained_models/1_lstm_ja_en_00.hdf5\",\n",
    "    \"translate_length\":25,\n",
    "    \"en_W2V_FILE\" : \"../data/GoogleNews-vectors-negative300.bin.gz\",\n",
    "    \"jp_W2V_FILE\":\"../data/ja_data/ja.bin\",\n",
    "    \"src\":\"en\",\n",
    "    \"trg\":\"jp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFjiCn22blbf"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,config):\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.validation_split = config[\"validation_split\"]\n",
    "        self.trained_param_dir = config[\"trained_param_dir\"]\n",
    "        self.output_dim = config[\"output_dim\"]\n",
    "        self.hist =None\n",
    "    def train(self,e_input,d_input,target):\n",
    "        print(\"#1 train procedure start\")\n",
    "        model,_,_ = build_model(config)\n",
    "        model.summary()\n",
    "        \n",
    "        if os.path.isfile(self.trained_param_dir) and False: #モデルの学習済みパラメータ\n",
    "            print(\"2-1? load param\")\n",
    "            model.load_weights(self.trained_param_dir)\n",
    "        else:\n",
    "            print(\"no_emb\")\n",
    "        print(\"#6 start training\")\n",
    "        \n",
    "        target_categorical = np_utils.to_categorical(output_target_padded,self.output_dim)\n",
    "       \n",
    "        self.hist=model.fit([e_input,d_input],target_categorical,epochs=self.epochs,batch_size=self.batch_size,validation_split=self.validation_split)\n",
    "        print(\"#9 save_param\")\n",
    "        model.save_weights(self.trained_param_dir)\n",
    "        #return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self,config):\n",
    "        self.translate_length = config[\"translate_length\"]\n",
    "        self.trained_param_dir = config[\"trained_param_dir\"]\n",
    "        self.model,self.encoder,self.decoder = build_model(config,test=True)\n",
    "        self.model.load_weights(self.trained_param_dir)\n",
    "    ## 翻訳文生成\n",
    "    def _translate(self,e_input):\n",
    "        #encode input to vec\n",
    "        #encoder_outputs,state_h_1,state_c_1 = self.encoder.predict(e_input)\n",
    "        #states_values=[state_h_1,state_c_1]\n",
    "        encoder_outputs,*states_values = self.encoder.predict(e_input)\n",
    "        \n",
    "        #first token\n",
    "        target_seq=np.zeros((1,1))\n",
    "        target_seq[0,0] = config[\"SOS_token\"]\n",
    "        \n",
    "        decoded_sentence=[]\n",
    "        for i in range(0,self.translate_length):\n",
    "            #output_tokens,h1,c1 = self.decoder.predict([target_seq]+states_values)\n",
    "            output_tokens,*states_values = self.decoder.predict([target_seq]+states_values)\n",
    "            \n",
    "            sampled_token_index=np.argmax(output_tokens[0,0,:])\n",
    "            if sampled_token_index==config[\"EOS_token\"]:\n",
    "                decoded_sentence.append(config[\"EOS_token\"])\n",
    "                break\n",
    "            else:\n",
    "                target_seq[0,0] = sampled_token_index\n",
    "                #states_values =[h1,c1]\n",
    "                decoded_sentence.append(sampled_token_index)\n",
    "        return decoded_sentence                                    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def translate_demo(self,src_data_id_seq):\n",
    "        ret=[]\n",
    "        for src in src_data_id_seq:\n",
    "            id_seq_mat = np.array([src])\n",
    "            pred_id_padded = sequence.pad_sequences(id_seq_mat,maxlen=config[\"MAX_LENGTH\"],padding=\"post\",truncating=\"post\")\n",
    "            pred=self._translate(pred_id_padded)\n",
    "            ret.append(pred)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_en_emb(config):\n",
    "    en_word2vec= KeyedVectors.load_word2vec_format(config[\"en_W2V_FILE\"],binary=True)\n",
    "    en_EMBEDDING_DIM=config[\"emb_dim\"]\n",
    "    #n_word<max_featureの時にerrになるよ\n",
    "    vocabulary_size=min(EN_lang.n_words,config[\"max_features\"])\n",
    "    en_embedding_matrix = np.zeros((vocabulary_size, en_EMBEDDING_DIM))\n",
    "    print(\"voc->\",vocabulary_size)\n",
    "    cnt=0\n",
    "    for word, i in EN_lang.word2index.items():\n",
    "        if   i==0 or i==1 or i ==2:\n",
    "            continue\n",
    "        try:\n",
    "            en_embedding_vector = en_word2vec[word]\n",
    "            en_embedding_matrix[i] = en_embedding_vector\n",
    "        except KeyError:\n",
    "            cnt+=1\n",
    "            en_embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25),en_EMBEDDING_DIM)\n",
    "    print(\"UNK_rate\",cnt/i)\n",
    "    del en_word2vec\n",
    "    gc.collect()\n",
    "    return en_embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "QA-6hirSN7c3",
    "outputId": "e9b07f89-37e4-4be1-e085-9e7cc53a4a20"
   },
   "outputs": [],
   "source": [
    "def build_jp_emb(config):\n",
    "    jp_word2vec= model = Word2Vec.load(config[\"jp_W2V_FILE\"])\n",
    "    jp_EMBEDDING_DIM=config[\"emb_dim\"]\n",
    "    vocabulary_size=min(JP_lang.n_words,config[\"max_features\"])\n",
    "    jp_embedding_matrix = np.zeros((vocabulary_size, jp_EMBEDDING_DIM))\n",
    "    print(\"voc->\",vocabulary_size)\n",
    "    cnt=0\n",
    "    for word, i in JP_lang.word2index.items():\n",
    "        if   i==0 or i==1 or i ==2:\n",
    "            continue\n",
    "        try:\n",
    "            jp_embedding_vector = jp_word2vec[word]\n",
    "            jp_embedding_matrix[i] = jp_embedding_vector\n",
    "        except KeyError:\n",
    "            cnt+=1\n",
    "            jp_embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25),jp_EMBEDDING_DIM)\n",
    "    print(\"UNK/rate->\",cnt/i)\n",
    "\n",
    "    del jp_word2vec\n",
    "    gc.collect()\n",
    "    return jp_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5h6V9OfqpcQn"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YmVIGLgYpdn2",
    "outputId": "703c3b2f-347a-4377-af52-f1ce4717b447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading lines\n"
     ]
    }
   ],
   "source": [
    "data=loadLangs(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_FCy7THnpdim"
   },
   "outputs": [],
   "source": [
    "val_data = data[config[\"train_size\"]:config[\"train_size\"]+config[\"val_size\"]]\n",
    "data = data[:config[\"train_size\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fsa0heCPpow7"
   },
   "outputs": [],
   "source": [
    "EN_lang = LangEn(config)\n",
    "JP_lang = LangJa(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWurNSGQpoug"
   },
   "outputs": [],
   "source": [
    "for s in data[config[\"en_col\"]]:\n",
    "    EN_lang.addSentence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJgN_yKjposQ"
   },
   "outputs": [],
   "source": [
    "for s in data[config[\"jp_col\"]]:\n",
    "    JP_lang.addSentence(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5Tz-dt4YJUu"
   },
   "source": [
    "## input の加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"src\"]==\"jp\":\n",
    "    src_col=config[\"jp_col\"]\n",
    "    trg_col=config[\"en_col\"]\n",
    "    Langs={\"src\":JP_lang,\"trg\":EN_lang}\n",
    "else:\n",
    "    src_col=config[\"en_col\"]\n",
    "    trg_col=config[\"jp_col\"]\n",
    "    Langs={\"trg\":JP_lang,\"src\":EN_lang}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZkMItuoprY2"
   },
   "outputs": [],
   "source": [
    "input_en = data[src_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6IBzGaKvpYS"
   },
   "outputs": [],
   "source": [
    "input_source_lang=data[src_col].apply(lambda x:Langs[\"src\"].word2id(x))\n",
    "input_target_lang=data[trg_col].apply(lambda x:Langs[\"trg\"].word2id(x,target=True))\n",
    "output_target_lang=data[trg_col].apply(lambda x:Langs[\"trg\"].word2id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNv2-2QGwJOP"
   },
   "outputs": [],
   "source": [
    "input_source_padded=sequence.pad_sequences(input_source_lang,maxlen=config[\"MAX_LENGTH\"],padding=\"post\",truncating=\"post\")\n",
    "input_target_padded=sequence.pad_sequences(input_target_lang,maxlen=config[\"MAX_LENGTH\"],padding=\"post\",truncating=\"post\")\n",
    "output_target_padded=sequence.pad_sequences(output_target_lang,maxlen=config[\"MAX_LENGTH\"],padding=\"post\",truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 14:16:14.857139 140508227524416 deprecation_wrapper.py:119] From /home/ueki.k/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0802 14:16:14.924930 140508227524416 deprecation_wrapper.py:119] From /home/ueki.k/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0802 14:16:14.926877 140508227524416 deprecation_wrapper.py:119] From /home/ueki.k/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 train procedure start\n",
      "#3 encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:16:15.118792 140508227524416 deprecation_wrapper.py:119] From /home/ueki.k/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#4 decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:16:35.984553 140508227524416 deprecation_wrapper.py:119] From /home/ueki.k/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:16:36.204930 140508227524416 deprecation_wrapper.py:119] From /home/ueki.k/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#6\n",
      "#7\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 300)      1500000     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 300)      1200        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 300)      1500000     decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_LSTM_fw1 (LSTM)         [(None, 20, 400), (N 1121600     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_LSTM_bw1 (LSTM)         [(None, 20, 400), (N 1121600     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 300)      1200        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 400)          0           encoder_LSTM_fw1[0][1]           \n",
      "                                                                 encoder_LSTM_bw1[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 400)          0           encoder_LSTM_fw1[0][2]           \n",
      "                                                                 encoder_LSTM_bw1[0][2]           \n",
      "__________________________________________________________________________________________________\n",
      "decode_LSTM1 (LSTM)             [(None, 20, 400), (N 1121600     batch_normalization_2[0][0]      \n",
      "                                                                 add_2[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_Dense (Dense)           (None, 20, 5000)     2005000     decode_LSTM1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 8,372,200\n",
      "Trainable params: 8,371,000\n",
      "Non-trainable params: 1,200\n",
      "__________________________________________________________________________________________________\n",
      "no_emb\n",
      "#6 start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:16:37.581124 140508227524416 deprecation.py:323] From /home/ueki.k/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13500 samples, validate on 1500 samples\n",
      "Epoch 1/1\n",
      "13500/13500 [==============================] - 116s 9ms/step - loss: 2.7990 - categorical_accuracy: 0.5891 - val_loss: 2.0641 - val_categorical_accuracy: 0.6479\n",
      "#9 save_param\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`save_weights` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-218b2b5775ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_source_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_target_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_target_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d9f3c887e880>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, e_input, d_input, target)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#9 save_param\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_param_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m#return model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \"\"\"\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`save_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# If file exists and should not be overwritten:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_weights` requires h5py."
     ]
    }
   ],
   "source": [
    "trainer.train(input_source_padded,input_target_padded,output_target_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m8_2NQDMYRrV",
    "outputId": "577b1ea5-a7c7-4601-983f-e29ad73d26ba"
   },
   "outputs": [],
   "source": [
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5khAA1aymGoX"
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9N9v_wwtqu6E"
   },
   "outputs": [],
   "source": [
    "val_data_id = val_data[src_col].apply(lambda x:Langs[\"src\"].word2id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "aA--RWDUHjPB",
    "outputId": "ac683833-7751-43e7-fd89-01a41abff1e4"
   },
   "outputs": [],
   "source": [
    "val_data_id[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = translator.translate_demo(val_data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0w7srpDEx_s"
   },
   "outputs": [],
   "source": [
    "for src,pred,target in zip(val_data[src_col],ret,val_data[trg_col]):\n",
    "    print(\"src->\",src)\n",
    "    print()\n",
    "    print(\"pred->\",\" \".join(Langs[\"trg\"].id2word(pred)))\n",
    "    print(\"ans->\",target)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drEeGLeON8Jj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1-LSTM",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
