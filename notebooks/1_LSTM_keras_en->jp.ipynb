{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os\n",
    "sys.path.append(\"/Users/ueki/Desktop/work/jp_en_translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from models.Seq2Seq_1 import build_model\n",
    "from utils.LangEn import LangEn\n",
    "from utils.LangJa import LangJa\n",
    "from utils.preprocess import loadLangs\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"corpus_file\":\"../data/jpn.txt\",\n",
    "    \"en_col\":\"description_en\",\n",
    "    \"jp_col\":\"description_jp\",\n",
    "    \"SOS_token\":1,\n",
    "    \"EOS_token\":0,\n",
    "    \"UNK_token\":2,\n",
    "    \"max_features\":5000,\n",
    "    \"MAX_LENGTH\":20,\n",
    "    \"train_size\":15000,\n",
    "    \"val_size\":100,\n",
    "    \"batch_size\":128,\n",
    "    \"epochs\":20,\n",
    "    \"maxlen_enc\":20,\n",
    "    \"maxlen_dec\":20,\n",
    "    \"n_hidden\":400,\n",
    "    \"input_dim\":5000,\n",
    "    \"output_dim\":5000,\n",
    "    \"emb_dim\":300,\n",
    "    \"use_enc_emb\":False,\n",
    "    \"use_dec_emb\":False,\n",
    "    \"validation_split\":0.1,\n",
    "    \"trained_param_dir\":\"../trained_models/1_lstm_ja_en_01.hdf5\",\n",
    "    \"translate_length\":25,\n",
    "    \"en_W2V_FILE\" : \"../data/GoogleNews-vectors-negative300.bin.gz\",\n",
    "    \"jp_W2V_FILE\":\"../data/ja_data/ja.bin\",\n",
    "    \"src\":\"en\",\n",
    "    \"trg\":\"jp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFjiCn22blbf"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,config):\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.validation_split = config[\"validation_split\"]\n",
    "        self.trained_param_dir = config[\"trained_param_dir\"]\n",
    "        self.output_dim = config[\"output_dim\"]\n",
    "        self.hist =None\n",
    "    def train(self,e_input,d_input,target):\n",
    "        print(\"#1 train procedure start\")\n",
    "        model,_,_ = build_model(config)\n",
    "        model.summary()\n",
    "        \n",
    "        if os.path.isfile(self.trained_param_dir) and False: #モデルの学習済みパラメータ\n",
    "            print(\"2-1? load param\")\n",
    "            model.load_weights(self.trained_param_dir)\n",
    "        else:\n",
    "            print(\"no_emb\")\n",
    "        print(\"#6 start training\")\n",
    "        \n",
    "        target_categorical = np_utils.to_categorical(output_target_padded,self.output_dim)\n",
    "       \n",
    "        self.hist=model.fit([e_input,d_input],target_categorical,epochs=self.epochs,batch_size=self.batch_size,validation_split=self.validation_split)\n",
    "        print(\"#9 save_param\")\n",
    "        model.save_weights(self.trained_param_dir)\n",
    "        #return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self,config):\n",
    "        self.translate_length = config[\"translate_length\"]\n",
    "        self.trained_param_dir = config[\"trained_param_dir\"]\n",
    "        self.model,self.encoder,self.decoder = build_model(config,test=True)\n",
    "        self.model.load_weights(self.trained_param_dir)\n",
    "    ## 翻訳文生成\n",
    "    def _translate(self,e_input):\n",
    "        #encode input to vec\n",
    "        #encoder_outputs,state_h_1,state_c_1 = self.encoder.predict(e_input)\n",
    "        #states_values=[state_h_1,state_c_1]\n",
    "        encoder_outputs,*states_values = self.encoder.predict(e_input)\n",
    "        \n",
    "        #first token\n",
    "        target_seq=np.zeros((1,1))\n",
    "        target_seq[0,0] = config[\"SOS_token\"]\n",
    "        \n",
    "        decoded_sentence=[]\n",
    "        for i in range(0,self.translate_length):\n",
    "            #output_tokens,h1,c1 = self.decoder.predict([target_seq]+states_values)\n",
    "            output_tokens,*states_values = self.decoder.predict([target_seq]+states_values)\n",
    "            \n",
    "            sampled_token_index=np.argmax(output_tokens[0,0,:])\n",
    "            if sampled_token_index==config[\"EOS_token\"]:\n",
    "                decoded_sentence.append(config[\"EOS_token\"])\n",
    "                break\n",
    "            else:\n",
    "                target_seq[0,0] = sampled_token_index\n",
    "                #states_values =[h1,c1]\n",
    "                decoded_sentence.append(sampled_token_index)\n",
    "        return decoded_sentence                                    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def translate_demo(self,src_data_id_seq):\n",
    "        ret=[]\n",
    "        for src in src_data_id_seq:\n",
    "            id_seq_mat = np.array([src])\n",
    "            pred_id_padded = sequence.pad_sequences(id_seq_mat,maxlen=config[\"MAX_LENGTH\"],padding=\"post\",truncating=\"post\")\n",
    "            pred=self._translate(pred_id_padded)\n",
    "            ret.append(pred)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_en_emb(config):\n",
    "    en_word2vec= KeyedVectors.load_word2vec_format(config[\"en_W2V_FILE\"],binary=True)\n",
    "    en_EMBEDDING_DIM=config[\"emb_dim\"]\n",
    "    #n_word<max_featureの時にerrになるよ\n",
    "    vocabulary_size=min(EN_lang.n_words,config[\"max_features\"])\n",
    "    en_embedding_matrix = np.zeros((vocabulary_size, en_EMBEDDING_DIM))\n",
    "    print(\"voc->\",vocabulary_size)\n",
    "    cnt=0\n",
    "    for word, i in EN_lang.word2index.items():\n",
    "        if   i==0 or i==1 or i ==2:\n",
    "            continue\n",
    "        try:\n",
    "            en_embedding_vector = en_word2vec[word]\n",
    "            en_embedding_matrix[i] = en_embedding_vector\n",
    "        except KeyError:\n",
    "            cnt+=1\n",
    "            en_embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25),en_EMBEDDING_DIM)\n",
    "    print(\"UNK_rate\",cnt/i)\n",
    "    del en_word2vec\n",
    "    gc.collect()\n",
    "    return en_embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "QA-6hirSN7c3",
    "outputId": "e9b07f89-37e4-4be1-e085-9e7cc53a4a20"
   },
   "outputs": [],
   "source": [
    "def build_jp_emb(config):\n",
    "    jp_word2vec= model = Word2Vec.load(config[\"jp_W2V_FILE\"])\n",
    "    jp_EMBEDDING_DIM=config[\"emb_dim\"]\n",
    "    vocabulary_size=min(JP_lang.n_words,config[\"max_features\"])\n",
    "    jp_embedding_matrix = np.zeros((vocabulary_size, jp_EMBEDDING_DIM))\n",
    "    print(\"voc->\",vocabulary_size)\n",
    "    cnt=0\n",
    "    for word, i in JP_lang.word2index.items():\n",
    "        if   i==0 or i==1 or i ==2:\n",
    "            continue\n",
    "        try:\n",
    "            jp_embedding_vector = jp_word2vec[word]\n",
    "            jp_embedding_matrix[i] = jp_embedding_vector\n",
    "        except KeyError:\n",
    "            cnt+=1\n",
    "            jp_embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25),jp_EMBEDDING_DIM)\n",
    "    print(\"UNK/rate->\",cnt/i)\n",
    "\n",
    "    del jp_word2vec\n",
    "    gc.collect()\n",
    "    return jp_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5h6V9OfqpcQn"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YmVIGLgYpdn2",
    "outputId": "703c3b2f-347a-4377-af52-f1ce4717b447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading lines\n"
     ]
    }
   ],
   "source": [
    "data=loadLangs(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_FCy7THnpdim"
   },
   "outputs": [],
   "source": [
    "val_data = data[config[\"train_size\"]:config[\"train_size\"]+config[\"val_size\"]]\n",
    "data = data[:config[\"train_size\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fsa0heCPpow7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 14:31:17.561068 4685845952 toolwrapper.py:77] stdbuf was not found; communication with perl may hang due to stdio buffering.\n"
     ]
    }
   ],
   "source": [
    "EN_lang = LangEn(config)\n",
    "JP_lang = LangJa(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWurNSGQpoug"
   },
   "outputs": [],
   "source": [
    "for s in data[config[\"en_col\"]]:\n",
    "    EN_lang.addSentence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJgN_yKjposQ"
   },
   "outputs": [],
   "source": [
    "for s in data[config[\"jp_col\"]]:\n",
    "    JP_lang.addSentence(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5Tz-dt4YJUu"
   },
   "source": [
    "## input の加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"src\"]==\"jp\":\n",
    "    src_col=config[\"jp_col\"]\n",
    "    trg_col=config[\"en_col\"]\n",
    "    Langs={\"src\":JP_lang,\"trg\":EN_lang}\n",
    "else:\n",
    "    src_col=config[\"en_col\"]\n",
    "    trg_col=config[\"jp_col\"]\n",
    "    Langs={\"trg\":JP_lang,\"src\":EN_lang}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZkMItuoprY2"
   },
   "outputs": [],
   "source": [
    "input_en = data[src_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6IBzGaKvpYS"
   },
   "outputs": [],
   "source": [
    "input_source_lang=data[src_col].apply(lambda x:Langs[\"src\"].word2id(x))\n",
    "input_target_lang=data[trg_col].apply(lambda x:Langs[\"trg\"].word2id(x,target=True))\n",
    "output_target_lang=data[trg_col].apply(lambda x:Langs[\"trg\"].word2id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNv2-2QGwJOP"
   },
   "outputs": [],
   "source": [
    "input_source_padded=sequence.pad_sequences(input_source_lang,maxlen=config[\"MAX_LENGTH\"],padding=\"post\",truncating=\"post\")\n",
    "input_target_padded=sequence.pad_sequences(input_target_lang,maxlen=config[\"MAX_LENGTH\"],padding=\"post\",truncating=\"post\")\n",
    "output_target_padded=sequence.pad_sequences(output_target_lang,maxlen=config[\"MAX_LENGTH\"],padding=\"post\",truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:31:20.242176 4685845952 deprecation_wrapper.py:119] From /Users/ueki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0802 14:31:20.263305 4685845952 deprecation_wrapper.py:119] From /Users/ueki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0802 14:31:20.265338 4685845952 deprecation_wrapper.py:119] From /Users/ueki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0802 14:31:20.383045 4685845952 deprecation_wrapper.py:119] From /Users/ueki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 train procedure start\n",
      "#3 encoder\n",
      "#4 decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:31:21.650373 4685845952 deprecation_wrapper.py:119] From /Users/ueki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0802 14:31:21.771257 4685845952 deprecation_wrapper.py:119] From /Users/ueki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#5\n",
      "#6\n",
      "#7\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 300)      1500000     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 300)      1200        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 300)      1500000     decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_LSTM_fw1 (LSTM)         [(None, 20, 400), (N 1121600     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_LSTM_bw1 (LSTM)         [(None, 20, 400), (N 1121600     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 300)      1200        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 400)          0           encoder_LSTM_fw1[0][1]           \n",
      "                                                                 encoder_LSTM_bw1[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 400)          0           encoder_LSTM_fw1[0][2]           \n",
      "                                                                 encoder_LSTM_bw1[0][2]           \n",
      "__________________________________________________________________________________________________\n",
      "decode_LSTM1 (LSTM)             [(None, 20, 400), (N 1121600     batch_normalization_2[0][0]      \n",
      "                                                                 add_2[0][0]                      \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_Dense (Dense)           (None, 20, 5000)     2005000     decode_LSTM1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 8,372,200\n",
      "Trainable params: 8,371,000\n",
      "Non-trainable params: 1,200\n",
      "__________________________________________________________________________________________________\n",
      "no_emb\n",
      "#6 start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 14:31:22.597099 4685845952 deprecation.py:323] From /Users/ueki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13500 samples, validate on 1500 samples\n",
      "Epoch 1/20\n",
      "13500/13500 [==============================] - 212s 16ms/step - loss: 2.8330 - categorical_accuracy: 0.5909 - val_loss: 2.0598 - val_categorical_accuracy: 0.6481\n",
      "Epoch 2/20\n",
      "13500/13500 [==============================] - 238s 18ms/step - loss: 1.9802 - categorical_accuracy: 0.6603 - val_loss: 1.8392 - val_categorical_accuracy: 0.6720\n",
      "Epoch 3/20\n",
      "13500/13500 [==============================] - 225s 17ms/step - loss: 1.7825 - categorical_accuracy: 0.6829 - val_loss: 1.7184 - val_categorical_accuracy: 0.6864\n",
      "Epoch 4/20\n",
      "13500/13500 [==============================] - 234s 17ms/step - loss: 1.6430 - categorical_accuracy: 0.6989 - val_loss: 1.6526 - val_categorical_accuracy: 0.6934\n",
      "Epoch 5/20\n",
      "13500/13500 [==============================] - 226s 17ms/step - loss: 1.5332 - categorical_accuracy: 0.7121 - val_loss: 1.5957 - val_categorical_accuracy: 0.7037\n",
      "Epoch 6/20\n",
      "13500/13500 [==============================] - 233s 17ms/step - loss: 1.4325 - categorical_accuracy: 0.7259 - val_loss: 1.5617 - val_categorical_accuracy: 0.7085\n",
      "Epoch 7/20\n",
      "13500/13500 [==============================] - 219s 16ms/step - loss: 1.3383 - categorical_accuracy: 0.7398 - val_loss: 1.5257 - val_categorical_accuracy: 0.7173\n",
      "Epoch 8/20\n",
      "13500/13500 [==============================] - 292s 22ms/step - loss: 1.2467 - categorical_accuracy: 0.7542 - val_loss: 1.5022 - val_categorical_accuracy: 0.7209\n",
      "Epoch 9/20\n",
      "13500/13500 [==============================] - 268s 20ms/step - loss: 1.1622 - categorical_accuracy: 0.7671 - val_loss: 1.4859 - val_categorical_accuracy: 0.7245\n",
      "Epoch 10/20\n",
      "13500/13500 [==============================] - 230s 17ms/step - loss: 1.0810 - categorical_accuracy: 0.7801 - val_loss: 1.4850 - val_categorical_accuracy: 0.7234\n",
      "Epoch 11/20\n",
      "13500/13500 [==============================] - 230s 17ms/step - loss: 1.0016 - categorical_accuracy: 0.7940 - val_loss: 1.4709 - val_categorical_accuracy: 0.7283\n",
      "Epoch 12/20\n",
      "13500/13500 [==============================] - 218s 16ms/step - loss: 0.9250 - categorical_accuracy: 0.8074 - val_loss: 1.4734 - val_categorical_accuracy: 0.7282\n",
      "Epoch 13/20\n",
      "13500/13500 [==============================] - 217s 16ms/step - loss: 0.8518 - categorical_accuracy: 0.8213 - val_loss: 1.4760 - val_categorical_accuracy: 0.7288\n",
      "Epoch 14/20\n",
      "13500/13500 [==============================] - 230s 17ms/step - loss: 0.7841 - categorical_accuracy: 0.8342 - val_loss: 1.4804 - val_categorical_accuracy: 0.7294\n",
      "Epoch 15/20\n",
      "13500/13500 [==============================] - 252s 19ms/step - loss: 0.7216 - categorical_accuracy: 0.8467 - val_loss: 1.4890 - val_categorical_accuracy: 0.7284\n",
      "Epoch 16/20\n",
      "13500/13500 [==============================] - 325s 24ms/step - loss: 0.6591 - categorical_accuracy: 0.8596 - val_loss: 1.4981 - val_categorical_accuracy: 0.7298\n",
      "Epoch 17/20\n",
      "13500/13500 [==============================] - 256s 19ms/step - loss: 0.6025 - categorical_accuracy: 0.8714 - val_loss: 1.5174 - val_categorical_accuracy: 0.7290\n",
      "Epoch 18/20\n",
      "13500/13500 [==============================] - 212s 16ms/step - loss: 0.5505 - categorical_accuracy: 0.8829 - val_loss: 1.5320 - val_categorical_accuracy: 0.7304\n",
      "Epoch 19/20\n",
      "13500/13500 [==============================] - 209s 16ms/step - loss: 0.5010 - categorical_accuracy: 0.8936 - val_loss: 1.5446 - val_categorical_accuracy: 0.7300\n",
      "Epoch 20/20\n",
      "13500/13500 [==============================] - 216s 16ms/step - loss: 0.4543 - categorical_accuracy: 0.9042 - val_loss: 1.5637 - val_categorical_accuracy: 0.7321\n",
      "#9 save_param\n"
     ]
    }
   ],
   "source": [
    "trainer.train(input_source_padded,input_target_padded,output_target_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m8_2NQDMYRrV",
    "outputId": "577b1ea5-a7c7-4601-983f-e29ad73d26ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5khAA1aymGoX"
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9N9v_wwtqu6E"
   },
   "outputs": [],
   "source": [
    "val_data_id = val_data[src_col].apply(lambda x:Langs[\"src\"].word2id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "aA--RWDUHjPB",
    "outputId": "ac683833-7751-43e7-fd89-01a41abff1e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15043                        [206, 20, 560, 8, 1643, 5, 0]\n",
       "15044                    [25, 52, 8, 205, 358, 4270, 5, 0]\n",
       "15045        [10, 512, 351, 215, 3569, 32, 27, 2130, 5, 0]\n",
       "15046                       [18, 56, 1740, 397, 765, 5, 0]\n",
       "15047    [111, 44, 753, 65, 1793, 50, 455, 79, 2360, 5, 0]\n",
       "15048                    [175, 1875, 27, 2983, 2363, 5, 0]\n",
       "15049                    [170, 75, 44, 2, 614, 1219, 5, 0]\n",
       "15050             [490, 22, 44, 2, 2, 12, 144, 1109, 5, 0]\n",
       "15051                           [4, 56, 2, 111, 121, 5, 0]\n",
       "15052        [63, 792, 7, 584, 256, 16, 44, 591, 37, 5, 0]\n",
       "Name: description_en, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_id[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#3 encoder\n",
      "#4 decoder\n",
      "#5\n",
      "#6\n",
      "#7\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = translator.translate_demo(val_data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0w7srpDEx_s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src-> theres no need to hurry.\n",
      "\n",
      "pred-> 急が ない と 遅れる よ 。 EOS\n",
      "ans-> 急ぐ 必要 は あり ませ ん 。\n",
      "------------------\n",
      "src-> i want to join your band.\n",
      "\n",
      "pred-> UNK の 方 が UNK さ れ てる よ 。 EOS\n",
      "ans-> あなた の バンド に 入り たい な 。\n",
      "------------------\n",
      "src-> you must keep an eye on the child.\n",
      "\n",
      "pred-> その 本 は UNK に なり たい 。 EOS\n",
      "ans-> その 子 から 目 を 離さ ない よう に し なけれ ば いけ ない 。\n",
      "------------------\n",
      "src-> he is mad about music.\n",
      "\n",
      "pred-> 彼 は 音楽 が 好き で ある 。 EOS\n",
      "ans-> 彼 は 音楽 狂 だ 。\n",
      "------------------\n",
      "src-> with a little more patience she would have succeeded.\n",
      "\n",
      "pred-> 彼女 が そんな こと を し た ので 、 私 は 何 か 知っ て い まし た 。 EOS\n",
      "ans-> もし 彼女 が もう少し 我慢強かっ たら 、 成功 し て い た だろ う に 。\n",
      "------------------\n",
      "src-> they painted the fence green.\n",
      "\n",
      "pred-> 彼ら は UNK で UNK し て いる 。 EOS\n",
      "ans-> 彼ら は フェンス を 緑色 に 塗っ た 。\n",
      "------------------\n",
      "src-> there was a convention last month.\n",
      "\n",
      "pred-> UNK が UNK た 。 EOS\n",
      "ans-> 先月 、 集会 が あっ た 。\n",
      "------------------\n",
      "src-> shes not a fulltime employee of this company.\n",
      "\n",
      "pred-> 彼女 は UNK UNK も 全く 速く UNK 。 EOS\n",
      "ans-> 彼女 は この 会社 の 正社員 で は あり ませ ん 。\n",
      "------------------\n",
      "src-> tom is arguing with mary.\n",
      "\n",
      "pred-> トム は メアリー と UNK し て いる 。 EOS\n",
      "ans-> トム は メアリー と 喧嘩 し て いる 。\n",
      "------------------\n",
      "src-> it looks like today will be a long day.\n",
      "\n",
      "pred-> 雨 が 降っ て いる ので 、 私 は 犬 に 慣れ ます 。 EOS\n",
      "ans-> 今日 は 長い 一 日 に なり そう です 。\n",
      "------------------\n",
      "src-> tom cried all night long.\n",
      "\n",
      "pred-> トム は 一 日 中 泣い て いる 。 EOS\n",
      "ans-> トム は 一 晩 中 泣き明かし た 。\n",
      "------------------\n",
      "src-> tom didnt want it.\n",
      "\n",
      "pred-> トム は それ を する こと が でき ない 。 EOS\n",
      "ans-> トム は 欲しく なかっ た 。\n",
      "------------------\n",
      "src-> i was moved to tears by her speech.\n",
      "\n",
      "pred-> 私 は 彼女 に 会っ た とたん 、 UNK た 。 EOS\n",
      "ans-> 彼女 の スピーチ で 感動 し て 泣い た 。\n",
      "------------------\n",
      "src-> his wife was nowhere to be seen.\n",
      "\n",
      "pred-> 彼 の 部屋 は きちんと UNK て も 、 UNK し た 。 EOS\n",
      "ans-> 彼 の 妻 の 姿 は どこ に も 見え なかっ た 。\n",
      "------------------\n",
      "src-> in october i was in boston.\n",
      "\n",
      "pred-> その 歌 は よく 知っ て いる 。 EOS\n",
      "ans-> 10 月 は ボストン に い た 。\n",
      "------------------\n",
      "src-> i go to any party i am invited to.\n",
      "\n",
      "pred-> 私 が ここ に 来 て から 、 私 は 普通 は 外出 でき ない 。 EOS\n",
      "ans-> 私 は 招待 さ れ た パーティー に は 必ず 出席 する 。\n",
      "------------------\n",
      "src-> which player are you paying the most attention to this year?\n",
      "\n",
      "pred-> 今日 の 午後 は どう やっ て いく の は ？ EOS\n",
      "ans-> 今年 、 注目 し て いる 選手 は 誰 です か 。\n",
      "------------------\n",
      "src-> he earns more money than he can spend.\n",
      "\n",
      "pred-> 彼 は その 知らせ に 驚い た よう な 。 EOS\n",
      "ans-> 彼 は 使い きれ ない ほど の 金 を 稼ぐ 。\n",
      "------------------\n",
      "src-> i intend to try doing everything i can.\n",
      "\n",
      "pred-> 私 たち は 、 あなた が UNK と 思っ て い ます 。 EOS\n",
      "ans-> できる 限り の こと は し て みる つもり だ 。\n",
      "------------------\n",
      "src-> i dont want to tell you the truth.\n",
      "\n",
      "pred-> そんな こと を し て いる と 言う こと に は ない 。 EOS\n",
      "ans-> あなた に は 本当 の こと を 言い たく ない の 。\n",
      "------------------\n",
      "src-> will it hurt a lot?\n",
      "\n",
      "pred-> ひどく 痛い の ？ EOS\n",
      "ans-> かなり 痛む の です か 。\n",
      "------------------\n",
      "src-> i couldnt answer any questions on the test.\n",
      "\n",
      "pred-> もう 二 人 とも 正しい と 思う ん です 。 EOS\n",
      "ans-> テスト で 一 問 も 答え られ なかっ た 。\n",
      "------------------\n",
      "src-> tom talks like an old man.\n",
      "\n",
      "pred-> トム は 携帯 UNK の が 好き だ 。 EOS\n",
      "ans-> トム は 老人 の よう な 話し方 を する 。\n",
      "------------------\n",
      "src-> he likes to read books.\n",
      "\n",
      "pred-> 彼 は 本 を 読む の が 大好き だ 。 EOS\n",
      "ans-> 彼 は 本 を 読む の が 好き だ 。\n",
      "------------------\n",
      "src-> did you mistake the margarine for butter?\n",
      "\n",
      "pred-> あなた は １ ９ ８ ０ 年 の 主要 な 出来事 と 話し て い ます か 。 EOS\n",
      "ans-> マーガリン を バター と 間違え た の ？\n",
      "------------------\n",
      "src-> i thought nothing of it.\n",
      "\n",
      "pred-> それ は 本当 な こと を 聞い た 。 EOS\n",
      "ans-> なんて こと なかっ た 。\n",
      "------------------\n",
      "src-> parallel lines do not intersect each other.\n",
      "\n",
      "pred-> UNK は UNK と UNK する もの が ある 。 EOS\n",
      "ans-> 平行 線 は 交差 し ませ ん 。\n",
      "------------------\n",
      "src-> im actually a university teacher.\n",
      "\n",
      "pred-> 私 は 先生 に 怒ら れ た 。 EOS\n",
      "ans-> 正確 に 言う と 私 は 大学 講師 です 。\n",
      "------------------\n",
      "src-> ten years have passed since he died.\n",
      "\n",
      "pred-> 彼 が UNK に 入っ た 。 EOS\n",
      "ans-> 彼 が 死ん で から 十 年 に なり ます 。\n",
      "------------------\n",
      "src-> money does not always bring happiness.\n",
      "\n",
      "pred-> 多く の 人 が UNK を し て い ない ん です 。 EOS\n",
      "ans-> お金 が 幸福 を もたらす と は 限ら ない 。\n",
      "------------------\n",
      "src-> she was surprised at the news.\n",
      "\n",
      "pred-> 彼女 は その 知らせ に 驚い た 。 EOS\n",
      "ans-> 彼女 は その ニュース を 聞い て 驚い た 。\n",
      "------------------\n",
      "src-> he stabbed me in the back!\n",
      "\n",
      "pred-> 彼 は 私 に １ 時 に 帰っ て き た 。 EOS\n",
      "ans-> やつ は 僕 を 裏切っ た ん だ ！\n",
      "------------------\n",
      "src-> toms planning something special for marys birthday.\n",
      "\n",
      "pred-> トム の 誕生 日 の 名前 を 思い出せ ない 。 EOS\n",
      "ans-> トム は メアリー の 誕生 日 に 何 か 特別 な こと を 計画 し て いる 。\n",
      "------------------\n",
      "src-> he wrote the report.\n",
      "\n",
      "pred-> 彼 は UNK を UNK し た 。 EOS\n",
      "ans-> 彼 は 報告 書 を 作文 し た 。\n",
      "------------------\n",
      "src-> can i change the route?\n",
      "\n",
      "pred-> UNK を UNK て い ます か 。 EOS\n",
      "ans-> 路線 の 変更 は でき ます か 。\n",
      "------------------\n",
      "src-> he is at his desk.\n",
      "\n",
      "pred-> 彼 は その 本 を 読ん で いる 。 EOS\n",
      "ans-> 彼 は 机 に 向かっ て いる 。\n",
      "------------------\n",
      "src-> war began five years later.\n",
      "\n",
      "pred-> 嵐 は 作物 に 多大 な 影響 を 与え た 。 EOS\n",
      "ans-> 5 年 後 に 戦争 が 始まっ た 。\n",
      "------------------\n",
      "src-> they formed themselves into a circle.\n",
      "\n",
      "pred-> 彼ら は UNK UNK だっ た 。 EOS\n",
      "ans-> 彼ら は 輪 に なっ た 。\n",
      "------------------\n",
      "src-> i used to play tennis in high school.\n",
      "\n",
      "pred-> 私 は いつも 学校 に 遅れる かも しれ ない 。 EOS\n",
      "ans-> 高校 時代 は よく テニス を し た もの です 。\n",
      "------------------\n",
      "src-> it was such a shock.\n",
      "\n",
      "pred-> それ が UNK だっ た 。 EOS\n",
      "ans-> それ は たいへん な ショック でし た 。\n",
      "------------------\n",
      "src-> whose umbrella is this?\n",
      "\n",
      "pred-> これ 誰 の ？ EOS\n",
      "ans-> これ 誰 の 傘 ？\n",
      "------------------\n",
      "src-> show me how.\n",
      "\n",
      "pred-> あなた の UNK を 見 て くれ ます 。 EOS\n",
      "ans-> どう やっ て やる の か 教え て 。\n",
      "------------------\n",
      "src-> did anybody get injured?\n",
      "\n",
      "pred-> 誰 か 怪我 し た ？ EOS\n",
      "ans-> 誰 か 怪我 し た ？\n",
      "------------------\n",
      "src-> if you calculate the electric field using this equation the result comes out like the following.\n",
      "\n",
      "pred-> 「 私 は 今日 の 午後 、 被災 し て 初めて だ から 、 彼ら が UNK まし た 。 EOS\n",
      "ans-> この 式 によって 電場 を 計算 し て やる と 、 結果 は 次 の よう に なる 。\n",
      "------------------\n",
      "src-> the travelers came from many lands.\n",
      "\n",
      "pred-> UNK に UNK が 鳴っ た 。 EOS\n",
      "ans-> 旅行 者 達 は いろいろ な 国 から やって来 た 。\n",
      "------------------\n",
      "src-> communications broke down.\n",
      "\n",
      "pred-> UNK を UNK 。 EOS\n",
      "ans-> 通信 手段 が 機能 し なく なっ た 。\n",
      "------------------\n",
      "src-> the sunny side of the hill is full of deciduous trees.\n",
      "\n",
      "pred-> １ ９ ９ ８ 年 前 の UNK は １ 日 に UNK れ て いる 。 EOS\n",
      "ans-> 丘 の 日 が 当っ て いる 部分 は 落葉樹 で いっぱい だ 。\n",
      "------------------\n",
      "src-> can you tell me how to get to the station?\n",
      "\n",
      "pred-> 駅 まで の 行き 方 を 教え て い た だけ ませ ん か 。 EOS\n",
      "ans-> 駅 へ どう 行っ たら 良い か を 教え て もらえ ませ ん か 。\n",
      "------------------\n",
      "src-> the old man was loved by everyone.\n",
      "\n",
      "pred-> その 知らせ に その 知らせ は UNK た 。 EOS\n",
      "ans-> その 老人 は 皆 に 愛さ れ て い た 。\n",
      "------------------\n",
      "src-> he pinched and scraped for many years to save money.\n",
      "\n",
      "pred-> 彼 は UNK に UNK を UNK う と 努力 し て いる 。 EOS\n",
      "ans-> 彼 は 金 を ためる ため 何 年間 も けちけち 倹約 し た 。\n",
      "------------------\n",
      "src-> the brain needs a continuous supply of blood.\n",
      "\n",
      "pred-> UNK は UNK やすい もの だ と 思う 。 EOS\n",
      "ans-> 小脳 は 血液 の 不断 の 供給 を 必要 と する 。\n",
      "------------------\n",
      "src-> his remark was really out of line.\n",
      "\n",
      "pred-> 彼 の UNK は UNK だ から 、 UNK 。 EOS\n",
      "ans-> 彼 の 意見 は 本当に 場違い だっ た 。\n",
      "------------------\n",
      "src-> im in trouble now.\n",
      "\n",
      "pred-> もう 一 回 やっ て み なさい 。 EOS\n",
      "ans-> 今 困っ て いる ん だ 。\n",
      "------------------\n",
      "src-> nobody knows whats going to happen.\n",
      "\n",
      "pred-> 誰 に も 言わ ない と 誓い ます か 。 EOS\n",
      "ans-> これから 何 が 起こる の か 、 誰 に も わから ない 。\n",
      "------------------\n",
      "src-> she has been watching television for three hours.\n",
      "\n",
      "pred-> 彼女 は 彼 に 会う の は 、 テレビ で 見 て い ます 。 EOS\n",
      "ans-> 彼女 は ３ 時間 テレビ を 見 続け て いる 。\n",
      "------------------\n",
      "src-> may i eat this bread?\n",
      "\n",
      "pred-> これ 、 UNK ない ん です か ？ EOS\n",
      "ans-> この パン 食べ て も いい ？\n",
      "------------------\n",
      "src-> see you.\n",
      "\n",
      "pred-> あなた に 会い まし た 。 EOS\n",
      "ans-> じゃ 、 また ねっ ！\n",
      "------------------\n",
      "src-> how long have you been looking for it?\n",
      "\n",
      "pred-> あなた は どこ に 住ん で いる の ？ EOS\n",
      "ans-> いつ から それ を お 探し です か 。\n",
      "------------------\n",
      "src-> the moon has set.\n",
      "\n",
      "pred-> UNK が 出 て い た 。 EOS\n",
      "ans-> 月 が 沈ん だ 。\n",
      "------------------\n",
      "src-> he looked into her eyes and suddenly went away.\n",
      "\n",
      "pred-> 彼女 は 目 を UNK て い た 。 EOS\n",
      "ans-> 彼 は 彼女 の 目 を 覗き 込む と 、 突然 立ち去っ た 。\n",
      "------------------\n",
      "src-> his grandmother looks healthy.\n",
      "\n",
      "pred-> 彼 の UNK は 好き で は あり ませ ん 。 EOS\n",
      "ans-> 彼 の おばあさん は 元気 そう です 。\n",
      "------------------\n",
      "src-> why do you love me?\n",
      "\n",
      "pred-> どうして あなた は 何 が 起こっ た ん です か ？ EOS\n",
      "ans-> なんで 私 の こと 好き な の ？\n",
      "------------------\n",
      "src-> i held on to the rope tightly so i wouldnt fall.\n",
      "\n",
      "pred-> 私 が 一 晩 中 踊る つもり だ と 言っ た 。 EOS\n",
      "ans-> 落ち ない よう に ロープ を 握りしめ た 。\n",
      "------------------\n",
      "src-> christmas is coming soon.\n",
      "\n",
      "pred-> もうすぐ クリスマス だ ね 。 EOS\n",
      "ans-> クリスマス が 近く なっ て き た 。\n",
      "------------------\n",
      "src-> we hurried to the train station.\n",
      "\n",
      "pred-> 私 たち が ここ に 駅 に 着い た 。 EOS\n",
      "ans-> 私 達 は 駅 へ 急い だ 。\n",
      "------------------\n",
      "src-> i have a good idea.\n",
      "\n",
      "pred-> いい 人 です 。 EOS\n",
      "ans-> いい 考え が あり ます 。\n",
      "------------------\n",
      "src-> theres nothing harder than a diamond.\n",
      "\n",
      "pred-> UNK は 、 UNK は まだ 生き て いる 。 EOS\n",
      "ans-> ダイヤモンド ほど 硬い もの は ない 。\n",
      "------------------\n",
      "src-> does it hurt a lot?\n",
      "\n",
      "pred-> ひどく 痛い の ？ EOS\n",
      "ans-> とっても 痛む ？\n",
      "------------------\n",
      "src-> thanks to his recommendation i was able to get a teaching job at a college in tokyo.\n",
      "\n",
      "pred-> 昔 の 友達 に 似 て い た が 、 私 が 大学 に 行き たい の を 見 て い た 。 EOS\n",
      "ans-> 彼 の 推薦 の おかげ で 、 私 は 東京 の 大学 で 教鞭 を とる こと が 出来 た 。\n",
      "------------------\n",
      "src-> theyre studying french and web design.\n",
      "\n",
      "pred-> 彼ら に は フランス語 が 話せる 人 を いる 。 EOS\n",
      "ans-> フランス語 と ウェブ・デザイン を 勉強 し て い ます 。\n",
      "------------------\n",
      "src-> five years have gone by since my father died.\n",
      "\n",
      "pred-> 父 が 私 の 仕事 を １ ０ ドル かかっ た 。 EOS\n",
      "ans-> 父 が 死ん で から 五 年 が 過ぎ た 。\n",
      "------------------\n",
      "src-> please write to me once in a while.\n",
      "\n",
      "pred-> 手紙 を 書い て ください 。 EOS\n",
      "ans-> たま に は 手紙 を 書い て ください 。\n",
      "------------------\n",
      "src-> mother has just gone shopping.\n",
      "\n",
      "pred-> 母 は 先週 大阪 を 行っ た 。 EOS\n",
      "ans-> 母 は ちょうど 買い物 に 出かけ た ところ です 。\n",
      "------------------\n",
      "src-> he was acquainted with everybody in town.\n",
      "\n",
      "pred-> 彼 は UNK に UNK を し た 。 EOS\n",
      "ans-> 彼 は 町 の 人 みんな と 付き合い が あっ た 。\n",
      "------------------\n",
      "src-> this bus can hold fifty people.\n",
      "\n",
      "pred-> この 歌 が 起こる の を 歌っ て いる 。 EOS\n",
      "ans-> この バス は 50 人 乗り です 。\n",
      "------------------\n",
      "src-> he loves traveling.\n",
      "\n",
      "pred-> 彼 は 目的 を 達成 し た 。 EOS\n",
      "ans-> 彼 は 旅行 が 大好き だ 。\n",
      "------------------\n",
      "src-> i should head out.\n",
      "\n",
      "pred-> UNK を し ます 。 EOS\n",
      "ans-> 出発 し なく て は いけ ない 。\n",
      "------------------\n",
      "src-> im afraid this data isnt reliable.\n",
      "\n",
      "pred-> 今日 の UNK は どこ に ある か わから ない 。 EOS\n",
      "ans-> この データ は 信用 でき ない と 思う 。\n",
      "------------------\n",
      "src-> a strange man was walking back and forth on the pavement.\n",
      "\n",
      "pred-> 警察 は その 事故 を UNK に 行っ た 。 EOS\n",
      "ans-> 見知らぬ 男 が 歩道 を 行っ たり 来 たり し て い た 。\n",
      "------------------\n",
      "src-> you have made all my dreams come true.\n",
      "\n",
      "pred-> あなた の UNK は 、 UNK に UNK て いる 。 EOS\n",
      "ans-> あなた は 私 の 夢 を 、 残らず 実現 さ せ て くれ た 。\n",
      "------------------\n",
      "src-> he lives in a large house.\n",
      "\n",
      "pred-> 彼 は 家 に い た ところ を UNK た 。 EOS\n",
      "ans-> 彼 は 広い 家 に 住ん で いる 。\n",
      "------------------\n",
      "src-> excuse me but could you lend me a pen?\n",
      "\n",
      "pred-> すみません が 、 ここ を UNK たい の です が 、 いかが です か 。 EOS\n",
      "ans-> すみません 、 ペン を 貸し て い た だけ ませ ん か ？\n",
      "------------------\n",
      "src-> were pretty pleased with that.\n",
      "\n",
      "pred-> 私 の UNK 、 UNK が 出 て いる 。 EOS\n",
      "ans-> 私 たち は それ で とても 満足 し て い ます 。\n",
      "------------------\n",
      "src-> stand still!\n",
      "\n",
      "pred-> 頭 が 痛い 。 EOS\n",
      "ans-> じっと し て ！\n",
      "------------------\n",
      "src-> could you please pass me the pepper?\n",
      "\n",
      "pred-> すみません が 胡椒 を とっ て 下さい ませ ん か 。 EOS\n",
      "ans-> コショウ を 取っ て もらえ ませ ん か 。\n",
      "------------------\n",
      "src-> tom and mary were dancing to the music.\n",
      "\n",
      "pred-> トム と メアリー は 二 人 とも 家 に いる 。 EOS\n",
      "ans-> トム と メアリー は 音楽 に 合わせ て 踊っ て い た 。\n",
      "------------------\n",
      "src-> nobody understands me.\n",
      "\n",
      "pred-> 誰 も い なかっ た 。 EOS\n",
      "ans-> 誰 も 私 の こと を 分かっ て くれ ない 。\n",
      "------------------\n",
      "src-> i dont know how to spell the word.\n",
      "\n",
      "pred-> 私 は あなた の UNK に は 、 何 が 起き て い ない の か 。 EOS\n",
      "ans-> その 単語 の スペル が わかり ませ ん 。\n",
      "------------------\n",
      "src-> i get the point.\n",
      "\n",
      "pred-> UNK は UNK だ 。 EOS\n",
      "ans-> 了解 し まし た 。\n",
      "------------------\n",
      "src-> he glanced at his watch.\n",
      "\n",
      "pred-> 彼 は UNK を UNK た 。 EOS\n",
      "ans-> 彼 は 時計 を チラッ と 見 た 。\n",
      "------------------\n",
      "src-> i cant stand him.\n",
      "\n",
      "pred-> 私 は 彼 に は 我慢 でき ない 。 EOS\n",
      "ans-> 私 は 彼 に がまん でき ない 。\n",
      "------------------\n",
      "src-> im sure i saw her two years ago.\n",
      "\n",
      "pred-> 私 は 彼女 の UNK に ぴったり UNK て い た 。 EOS\n",
      "ans-> 絶対 彼女 と 2 年 前 に 会っ てる と 思う 。\n",
      "------------------\n",
      "src-> he has an eye for art.\n",
      "\n",
      "pred-> 彼 は 仕事 を する 。 EOS\n",
      "ans-> 彼 は 絵 を 見る 目 が ある 。\n",
      "------------------\n",
      "src-> are there any cute girls in your class?\n",
      "\n",
      "pred-> どういう UNK だ よ ね ？ EOS\n",
      "ans-> クラス に かわいい 子 いる ？\n",
      "------------------\n",
      "src-> i made it myself.\n",
      "\n",
      "pred-> 私 は トム を 信じ て い ます 。 EOS\n",
      "ans-> 自分 で 作り まし た 。\n",
      "------------------\n",
      "src-> lets get started.\n",
      "\n",
      "pred-> さあ 、 UNK 。 EOS\n",
      "ans-> さあ 、 始め ましょ う 。\n",
      "------------------\n",
      "src-> take care.\n",
      "\n",
      "pred-> 気 を 付け て ね 。 EOS\n",
      "ans-> 気 を つけ て ！\n",
      "------------------\n",
      "src-> i dont know who he is.\n",
      "\n",
      "pred-> 彼 が 私 に は それ は 何 か する と 言っ た 。 EOS\n",
      "ans-> 彼 が 誰 だ か 知り ませ ん 。\n",
      "------------------\n",
      "src-> he remained silent.\n",
      "\n",
      "pred-> 彼 は 銀行 に 勤め て いる 。 EOS\n",
      "ans-> 彼 は 黙っ た まま だっ た 。\n",
      "------------------\n",
      "src-> dont lie. tell the truth.\n",
      "\n",
      "pred-> その こと について は 何 か 知っ て い ます 。 EOS\n",
      "ans-> うそ を つく な 、 正直 で あれ 。\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for src,pred,target in zip(val_data[src_col],ret,val_data[trg_col]):\n",
    "    print(\"src->\",src)\n",
    "    print()\n",
    "    print(\"pred->\",\" \".join(Langs[\"trg\"].id2word(pred)))\n",
    "    print(\"ans->\",target)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drEeGLeON8Jj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1-LSTM",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
